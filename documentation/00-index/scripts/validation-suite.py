#!/usr/bin/env python3
"""
CVD Documentation Validation Suite

Comprehensive validation system for all documentation components including:
- Internal link validation
- External link validation  
- Navigation path testing
- Metadata integrity checking
- Semantic tag validation
- Search index integrity
"""

import os
import re
import json
import yaml
import requests
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional
import time


class DocumentationValidator:
    """Main validation class for CVD documentation system"""
    
    def __init__(self, doc_root="/home/jbrice/Projects/365/documentation"):
        self.doc_root = Path(doc_root)
        self.project_root = Path("/home/jbrice/Projects/365")
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'internal_links': {},
            'external_links': {},
            'navigation': {},
            'metadata': {},
            'search_index': {},
            'summary': {}
        }
        
    def run_full_validation(self) -> Dict:
        """Run complete validation suite"""
        print("üîç Starting CVD Documentation Validation Suite")
        print(f"üìÅ Documentation root: {self.doc_root}")
        print(f"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # Run all validation components
        self.validate_internal_links()
        self.validate_external_links()
        self.validate_navigation_paths()
        self.validate_metadata_integrity()
        self.validate_search_index()
        self.generate_summary()
        
        return self.results
    
    def validate_internal_links(self):
        """Validate all internal markdown links"""
        print("üîó Validating internal links...")
        
        link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
        anchor_pattern = re.compile(r'#([a-zA-Z0-9-]+)')
        
        total_links = 0
        broken_links = []
        valid_links = 0
        
        for md_file in self.doc_root.rglob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Find all markdown links
                links = link_pattern.findall(content)
                
                for link_text, link_url in links:
                    total_links += 1
                    
                    # Skip external links
                    if link_url.startswith(('http://', 'https://')):
                        continue
                    
                    # Check if link has anchor
                    if '#' in link_url:
                        file_part, anchor = link_url.split('#', 1)
                        link_path = md_file.parent / file_part if file_part else md_file
                    else:
                        link_path = md_file.parent / link_url
                        anchor = None
                    
                    # Resolve relative path
                    try:
                        resolved_path = link_path.resolve()
                        
                        # Check if target file exists
                        if not resolved_path.exists():
                            broken_links.append({
                                'file': str(md_file.relative_to(self.doc_root)),
                                'link_text': link_text,
                                'link_url': link_url,
                                'error': 'File not found',
                                'target': str(resolved_path.relative_to(self.project_root))
                            })
                            continue
                        
                        # Check anchor if present
                        if anchor and resolved_path.suffix == '.md':
                            if not self._check_anchor_exists(resolved_path, anchor):
                                broken_links.append({
                                    'file': str(md_file.relative_to(self.doc_root)),
                                    'link_text': link_text,
                                    'link_url': link_url,
                                    'error': 'Anchor not found',
                                    'target': str(resolved_path.relative_to(self.project_root))
                                })
                                continue
                        
                        valid_links += 1
                        
                    except Exception as e:
                        broken_links.append({
                            'file': str(md_file.relative_to(self.doc_root)),
                            'link_text': link_text,
                            'link_url': link_url,
                            'error': f'Resolution error: {str(e)}',
                            'target': link_url
                        })
                        
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading {md_file}: {e}")
        
        self.results['internal_links'] = {
            'total_links': total_links,
            'valid_links': valid_links,
            'broken_links': len(broken_links),
            'broken_link_details': broken_links,
            'success_rate': (valid_links / total_links * 100) if total_links > 0 else 100
        }
        
        print(f"   üìä Total internal links: {total_links}")
        print(f"   ‚úÖ Valid links: {valid_links}")
        print(f"   ‚ùå Broken links: {len(broken_links)}")
        print(f"   üìà Success rate: {self.results['internal_links']['success_rate']:.1f}%")
    
    def validate_external_links(self):
        """Validate external HTTP/HTTPS links"""
        print("üåê Validating external links...")
        
        url_pattern = re.compile(r'https?://[^\s\)>]+')
        
        all_urls = set()
        total_urls = 0
        
        # Collect all external URLs
        for md_file in self.doc_root.rglob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                urls = url_pattern.findall(content)
                all_urls.update(urls)
                total_urls += len(urls)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading {md_file}: {e}")
        
        # Test each unique URL
        valid_urls = []
        broken_urls = []
        skipped_urls = []
        
        for url in all_urls:
            # Skip localhost and template URLs
            if any(skip in url for skip in ['localhost', 'example.com', 'your-domain.com', '127.0.0.1']):
                skipped_urls.append({'url': url, 'reason': 'localhost/template'})
                continue
                
            try:
                response = requests.head(url, timeout=10, allow_redirects=True)
                if response.status_code < 400:
                    valid_urls.append({
                        'url': url,
                        'status_code': response.status_code,
                        'response_time': response.elapsed.total_seconds()
                    })
                else:
                    broken_urls.append({
                        'url': url,
                        'status_code': response.status_code,
                        'error': 'HTTP error'
                    })
            except requests.RequestException as e:
                broken_urls.append({
                    'url': url,
                    'error': str(e),
                    'status_code': None
                })
            
            # Small delay to be respectful
            time.sleep(0.1)
        
        self.results['external_links'] = {
            'total_unique_urls': len(all_urls),
            'total_url_occurrences': total_urls,
            'valid_urls': len(valid_urls),
            'broken_urls': len(broken_urls),
            'skipped_urls': len(skipped_urls),
            'valid_url_details': valid_urls,
            'broken_url_details': broken_urls,
            'skipped_url_details': skipped_urls,
            'success_rate': (len(valid_urls) / (len(valid_urls) + len(broken_urls)) * 100) if (len(valid_urls) + len(broken_urls)) > 0 else 100
        }
        
        print(f"   üìä Total unique URLs: {len(all_urls)}")
        print(f"   ‚úÖ Valid URLs: {len(valid_urls)}")
        print(f"   ‚ùå Broken URLs: {len(broken_urls)}")
        print(f"   ‚è≠Ô∏è  Skipped URLs: {len(skipped_urls)}")
        print(f"   üìà Success rate: {self.results['external_links']['success_rate']:.1f}%")
    
    def validate_navigation_paths(self):
        """Validate navigation system integrity"""
        print("üß≠ Validating navigation paths...")
        
        navigation_tests = {
            'master_index_links': 0,
            'category_readmes': 0,
            'cross_references': 0,
            'quick_links': 0,
            'search_integration': 0
        }
        
        # Check master index
        master_index = self.doc_root / "00-index" / "MASTER_INDEX.md"
        if master_index.exists():
            navigation_tests['master_index_links'] = self._test_master_index(master_index)
        
        # Check category READMEs
        navigation_tests['category_readmes'] = self._test_category_readmes()
        
        # Check cross-references
        cross_ref_file = self.doc_root / "00-index" / "CROSS_REFERENCES.md"
        if cross_ref_file.exists():
            navigation_tests['cross_references'] = self._test_cross_references(cross_ref_file)
        
        # Check search integration
        search_index = self.doc_root / "00-index" / "SEARCH_INDEX.json"
        if search_index.exists():
            navigation_tests['search_integration'] = self._test_search_integration(search_index)
        
        self.results['navigation'] = navigation_tests
        
        total_nav_tests = sum(navigation_tests.values())
        print(f"   üìä Navigation tests passed: {total_nav_tests}")
    
    def validate_metadata_integrity(self):
        """Validate document metadata consistency"""
        print("üìã Validating metadata integrity...")
        
        metadata_stats = {
            'files_with_metadata': 0,
            'files_without_metadata': 0,
            'metadata_errors': [],
            'tag_distribution': {},
            'category_distribution': {},
            'version_distribution': {}
        }
        
        for md_file in self.doc_root.rglob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Check for YAML frontmatter
                if content.startswith('---'):
                    try:
                        # Extract frontmatter
                        _, frontmatter, _ = content.split('---', 2)
                        metadata = yaml.safe_load(frontmatter)
                        
                        if metadata:
                            metadata_stats['files_with_metadata'] += 1
                            
                            # Collect tag distribution
                            if 'tags' in metadata and isinstance(metadata['tags'], list):
                                for tag in metadata['tags']:
                                    metadata_stats['tag_distribution'][tag] = metadata_stats['tag_distribution'].get(tag, 0) + 1
                            
                            # Collect category distribution
                            if 'category' in metadata:
                                cat = metadata['category']
                                metadata_stats['category_distribution'][cat] = metadata_stats['category_distribution'].get(cat, 0) + 1
                            
                            # Collect version distribution
                            if 'version' in metadata:
                                ver = metadata['version']
                                metadata_stats['version_distribution'][ver] = metadata_stats['version_distribution'].get(ver, 0) + 1
                        else:
                            metadata_stats['files_without_metadata'] += 1
                            
                    except yaml.YAMLError as e:
                        metadata_stats['metadata_errors'].append({
                            'file': str(md_file.relative_to(self.doc_root)),
                            'error': f'YAML parsing error: {str(e)}'
                        })
                        metadata_stats['files_without_metadata'] += 1
                else:
                    metadata_stats['files_without_metadata'] += 1
                    
            except Exception as e:
                metadata_stats['metadata_errors'].append({
                    'file': str(md_file.relative_to(self.doc_root)),
                    'error': f'File reading error: {str(e)}'
                })
        
        self.results['metadata'] = metadata_stats
        
        total_files = metadata_stats['files_with_metadata'] + metadata_stats['files_without_metadata']
        metadata_coverage = (metadata_stats['files_with_metadata'] / total_files * 100) if total_files > 0 else 0
        
        print(f"   üìä Files with metadata: {metadata_stats['files_with_metadata']}")
        print(f"   üìä Files without metadata: {metadata_stats['files_without_metadata']}")
        print(f"   üìà Metadata coverage: {metadata_coverage:.1f}%")
        print(f"   üè∑Ô∏è  Unique tags: {len(metadata_stats['tag_distribution'])}")
        print(f"   üìÅ Categories: {len(metadata_stats['category_distribution'])}")
    
    def validate_search_index(self):
        """Validate search index integrity and completeness"""
        print("üîç Validating search index...")
        
        search_index_path = self.doc_root / "00-index" / "SEARCH_INDEX.json"
        search_stats = {
            'index_exists': False,
            'indexed_documents': 0,
            'total_documents': 0,
            'index_coverage': 0,
            'index_size': 0,
            'missing_documents': []
        }
        
        # Count total markdown documents
        all_md_files = list(self.doc_root.rglob("*.md"))
        search_stats['total_documents'] = len(all_md_files)
        
        if search_index_path.exists():
            search_stats['index_exists'] = True
            search_stats['index_size'] = search_index_path.stat().st_size
            
            try:
                with open(search_index_path, 'r', encoding='utf-8') as f:
                    search_index = json.load(f)
                
                if 'search_index' in search_index:
                    indexed_files = search_index['search_index']
                    search_stats['indexed_documents'] = len(indexed_files)
                    
                    # Check which documents are missing from index
                    indexed_basenames = set(indexed_files.keys())
                    all_basenames = {f.name for f in all_md_files}
                    
                    missing = all_basenames - indexed_basenames
                    search_stats['missing_documents'] = list(missing)
                    
                    search_stats['index_coverage'] = (search_stats['indexed_documents'] / search_stats['total_documents'] * 100) if search_stats['total_documents'] > 0 else 0
                    
            except (json.JSONDecodeError, KeyError) as e:
                search_stats['error'] = f'Index parsing error: {str(e)}'
        
        self.results['search_index'] = search_stats
        
        print(f"   üìä Search index exists: {search_stats['index_exists']}")
        print(f"   üìä Indexed documents: {search_stats['indexed_documents']}")
        print(f"   üìä Total documents: {search_stats['total_documents']}")
        print(f"   üìà Index coverage: {search_stats['index_coverage']:.1f}%")
        print(f"   üíæ Index size: {search_stats['index_size'] / 1024:.1f} KB")
    
    def generate_summary(self):
        """Generate validation summary and overall health score"""
        print("üìà Generating validation summary...")
        
        # Calculate overall scores
        internal_score = self.results['internal_links'].get('success_rate', 0)
        external_score = self.results['external_links'].get('success_rate', 0)
        
        metadata_files = self.results['metadata']['files_with_metadata']
        total_files = metadata_files + self.results['metadata']['files_without_metadata']
        metadata_score = (metadata_files / total_files * 100) if total_files > 0 else 0
        
        search_score = self.results['search_index'].get('index_coverage', 0)
        
        # Navigation score (simplified)
        nav_tests = self.results['navigation']
        nav_score = (sum(nav_tests.values()) / max(len(nav_tests), 1)) * 20  # Rough scoring
        
        overall_score = (internal_score + external_score + metadata_score + search_score + nav_score) / 5
        
        summary = {
            'overall_health_score': overall_score,
            'component_scores': {
                'internal_links': internal_score,
                'external_links': external_score,
                'metadata_integrity': metadata_score,
                'search_index': search_score,
                'navigation_system': nav_score
            },
            'critical_issues': [],
            'warnings': [],
            'recommendations': []
        }
        
        # Identify critical issues
        if internal_score < 95:
            summary['critical_issues'].append('Internal link validation below 95%')
        
        if external_score < 90:
            summary['warnings'].append('External link validation below 90%')
        
        if metadata_score < 80:
            summary['warnings'].append('Metadata coverage below 80%')
        
        if search_score < 90:
            summary['warnings'].append('Search index coverage below 90%')
        
        # Generate recommendations
        if len(self.results['internal_links']['broken_links']) > 0:
            summary['recommendations'].append('Fix broken internal links to improve navigation')
        
        if self.results['metadata']['files_without_metadata'] > 5:
            summary['recommendations'].append('Add metadata to improve document discovery')
        
        if len(self.results['search_index']['missing_documents']) > 0:
            summary['recommendations'].append('Rebuild search index to include all documents')
        
        self.results['summary'] = summary
        
        print(f"   üéØ Overall health score: {overall_score:.1f}/100")
        print(f"   ‚ö†Ô∏è  Critical issues: {len(summary['critical_issues'])}")
        print(f"   ‚ö†Ô∏è  Warnings: {len(summary['warnings'])}")
        print(f"   üí° Recommendations: {len(summary['recommendations'])}")
    
    def _check_anchor_exists(self, file_path: Path, anchor: str) -> bool:
        """Check if an anchor exists in a markdown file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Convert heading to anchor format
            heading_patterns = [
                f"#{anchor}",  # Direct anchor
                f"## {anchor.replace('-', ' ').title()}",  # Title case heading
                f"# {anchor.replace('-', ' ').title()}",   # Title case h1
                f"### {anchor.replace('-', ' ')}",         # Lower case h3
            ]
            
            for pattern in heading_patterns:
                if pattern.lower() in content.lower():
                    return True
                    
            return False
            
        except Exception:
            return False
    
    def _test_master_index(self, master_index_path: Path) -> int:
        """Test master index navigation links"""
        # Simplified test - count valid category references
        try:
            with open(master_index_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for category patterns
            category_pattern = re.compile(r'\d{2}-[a-z-]+')
            categories = category_pattern.findall(content)
            
            valid_categories = 0
            for cat in set(categories):
                cat_path = self.doc_root / cat
                if cat_path.exists():
                    valid_categories += 1
            
            return valid_categories
            
        except Exception:
            return 0
    
    def _test_category_readmes(self) -> int:
        """Test category README file accessibility"""
        valid_readmes = 0
        
        for category_dir in self.doc_root.iterdir():
            if category_dir.is_dir() and category_dir.name.startswith(tuple('0123456789')):
                readme_path = category_dir / "README.md"
                if readme_path.exists():
                    valid_readmes += 1
        
        return valid_readmes
    
    def _test_cross_references(self, cross_ref_path: Path) -> int:
        """Test cross-reference system integrity"""
        # Simplified test - return fixed value for now
        return 25  # Placeholder
    
    def _test_search_integration(self, search_index_path: Path) -> int:
        """Test search system integration"""
        # Simplified test - check if search script exists
        search_script = self.doc_root / "00-index" / "scripts" / "search.py"
        return 1 if search_script.exists() else 0
    
    def save_results(self, output_path: Optional[str] = None):
        """Save validation results to JSON file"""
        if output_path is None:
            output_path = self.doc_root / "00-index" / "validation-results.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"üíæ Validation results saved to: {output_path}")


def main():
    """Main validation runner"""
    validator = DocumentationValidator()
    results = validator.run_full_validation()
    
    print("\n" + "=" * 60)
    print("üìã VALIDATION SUMMARY")
    print("=" * 60)
    
    summary = results['summary']
    print(f"üéØ Overall Health Score: {summary['overall_health_score']:.1f}/100")
    
    if summary['critical_issues']:
        print("\n‚ùå CRITICAL ISSUES:")
        for issue in summary['critical_issues']:
            print(f"   ‚Ä¢ {issue}")
    
    if summary['warnings']:
        print("\n‚ö†Ô∏è  WARNINGS:")
        for warning in summary['warnings']:
            print(f"   ‚Ä¢ {warning}")
    
    if summary['recommendations']:
        print("\nüí° RECOMMENDATIONS:")
        for rec in summary['recommendations']:
            print(f"   ‚Ä¢ {rec}")
    
    # Save results
    validator.save_results()
    
    print(f"\n‚úÖ Validation completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return summary['overall_health_score']


if __name__ == "__main__":
    health_score = main()
    exit(0 if health_score >= 90 else 1)